from selenium import webdriver
import os
import sys
import pandas as pd
import numpy as np
from time import sleep # this should go at the top of the file
from bs4 import BeautifulSoup,SoupStrainer
from statistics import median
from statistics import mean
import re
import math
import matplotlib.pyplot as plt
from pylab import *

#driver declaration, comment this out when not scraping
#driver = webdriver.PhantomJS() # This is the driver to fetch the webpage

#fixed values, these are set and unchanged throughout
result_num = 50 #number of results to fetch on each page
result_num_str = str(result_num) #string version of result_num, i'll be using the int and str versions

#need to create a list of car makes to fetch, the last 2 digits are the 'id'
makes_list = (['toyota_88', 'volkswagen_93', 'nissan_61', 'hyundai_34', 'peugeot_67'])
#makes_list = (['peugeot_67'])

#function to find how many results are on each page and how many pages there are
def get_num_pages(url):
    
    driver.get(url)
    sleep(1)

    html_source = driver.page_source
    num_soup = BeautifulSoup(html_source,'html.parser')  
    # the page value is stored in a tag called "strong_tag"    
    for strong_tag in num_soup.findAll('strong'):
        pages = re.findall(r'\d+', strong_tag.text)
    
    #pages consists of the the total number of pages
    print(pages)
    return pages

 
def get_car_data(page, make_id, make_name):

    page_str = str(page)
    #the url used is formed by applying the page number and the make id
    
    url_clean = 'http://www.carsireland.ie/search-results.php?make_id=' + make_id + '&model_id=&per_page=' + result_num_str + '&page=' + page_str + '&min_year=1999&max_year=2014&search-used-submit=Search+Used+Cars'
   
    driver.get(url_clean)
    #a sleep is issused as much of the data is generated by JavaScript and can
    #take some time before it is completely rendered    
    sleep(1)
    
    html_source = driver.page_source    
    
    my_soup = BeautifulSoup(html_source,'html.parser')      
     
    #The required data is stored in different locations in the HTML
    #first I want the fuel type, engine size and colour of each entry
    fuel_list = []
    engine_size_list = []
    colour_list = []
    
    #beautiful soup does all the hard work of sorting the relevant structures
    for fuel in my_soup.findAll('span',{'class':'engine-fuel-type-hidden'}):
        fuel_list.append(fuel.text.strip())
    
    for engine_size in my_soup.findAll('span',{'class':'engine-size-for-search'}):
        engine_size_list.append(engine_size.text.strip())
    
    #the first table in the HTML contains all the data we need, ie the car adverts    
    tables = my_soup.findChildren('table')
    my_table = tables[0]
    rows = my_table.findChildren(['th', 'tr'])

    car_info = []
    for row in rows:
        # we want everything with a 'td' tag        
        cells = row.findChildren('td')
        for cell in cells: 
            value = cell.string
            cell_string = str(cell)
            if str(value) != 'None':
                #some entries have empty strings and we want to discard them here
                car_info.append(str(value)) 
            if "swatch" in cell_string:
                # swatch is the keyword that indicates the string containing the colour
                # we do a little bit of stripping to pull out the colour in the URL                
                sep = ' swatch'
                string_clean = cell_string.split(sep, 1)[0]
                sep = 't=\"'
                string_clean = string_clean.split(sep, 1)[1]
                colour_list.append(string_clean) 

    #I convert the list to a dataframe    
    colour_list_df = pd.DataFrame(colour_list)   
    
    #here I combine the fuel and engine lists
    fuel_and_engine_df = pd.concat([pd.DataFrame(fuel_list),pd.DataFrame(engine_size_list)], axis=1)
    
    # i need to reshape the matrix
    car_info = ([s.replace(',', '') for s in car_info]) # remove all the ,s  
    car_info_matrix = np.array(car_info).reshape(len(car_info)/4,4) 
    
    #then I need to tidy up the price column. The int value I want is surrounded
    #by a number of unwanted characters.
    car_info_matrix_price =  list(car_info_matrix[:,3])
    car_info_matrix_price = ([s.replace('â', '') for s in car_info_matrix_price])
    car_info_matrix_price = ([s.replace('‚', '') for s in car_info_matrix_price]) 
    car_info_matrix_price = ([s.replace('¬', '') for s in car_info_matrix_price])
    car_info_matrix_price = ([s.replace('€', '') for s in car_info_matrix_price])

    car_info_matrix = np.delete(car_info_matrix, np.s_[3], axis=1)
    car_info_matrix_price_np = np.array(car_info_matrix_price)
    #conver the price matrix to a dataframe    
    car_info_matrix_price_df = pd.DataFrame(car_info_matrix_price_np)    
    
    #convert all the other info to a dataframe
    car_info_matrix_df = pd.DataFrame(car_info_matrix)
    
    # i fill a full column with whatever the make we are working with
    makes_df = pd.DataFrame([make_name for x in range(50)])
    
    #i combine all the dataframes
    car_info_matrix_df = pd.concat([makes_df, car_info_matrix_df, fuel_and_engine_df, colour_list_df, car_info_matrix_price_df], axis=1)
    
    return car_info_matrix_df

def get_data(makes_list):
    local_car_info = []
    for make_val in makes_list:
        #i split the make strings to get the name and IDs
        #The IDs were found by doing manual searches on the site and recording the ones
        #I was interested in
        make_name = make_val.split('_')[0]
        make_id = make_val.split('_')[1]
        print(make_id)
        url = 'http://www.carsireland.ie/search-results.php?make_id=' + str(make_id) + '&model_id=&min_year=1999&max_year=2014&search-used-submit=Search+Used+Cars'    
        pages = get_num_pages(url)
        num_pages = math.floor(int(pages[2])/result_num)
        print ('Fetching number of results for:', make_name)    
        for page in range(num_pages):
            print ("Getting page %d of %d" % (page, num_pages))
            #for each page we take the results and add it to the list
            local_car_info.append(get_car_data(page, make_id, make_name))
    #for each make we add the list to the current list cerating one full list
    #with all the data
    full_car_info_new = pd.concat(local_car_info)
    #the indexes got a bit messed up when parsing so we fix that here
    full_car_info_new = full_car_info_new.reset_index(full_car_info_new)
    #drop the index column    
    full_car_info_new = full_car_info_new.drop(['index'],axis=1)
    #add in the correct column headers. Easier to do that here than earlier
    full_car_info_new.columns = ['make', 'county', 'mileage', 'year', 'fuel', 'enginesize', 'colour', 'price']
    ##need to do some tidying up
    #POA are listings without prices
    #Unspecified are listings where there is no mileage listed
    #'no colour' means no colour was listed
    #I want to remove all these entries
    #I also want to take out mileage values over 400k. In the age of the cars
    #we are looking at this is most likely a typo
    #same for mileages below 1k
    full_car_info_new = full_car_info_new[full_car_info_new.price != 'POA']
    full_car_info_new = full_car_info_new[full_car_info_new.mileage != 'Unspecified']
    full_car_info_new = full_car_info_new[full_car_info_new.colour != 'no colour\" height=\"16\" src=\"/images/swatches/null.gif\" width=\"16\"/></td>']
    full_car_info_new[['mileage']] = full_car_info_new[['mileage']].astype(int)
    full_car_info_new = full_car_info_new[full_car_info_new.mileage < 400000]
    full_car_info_new = full_car_info_new[full_car_info_new.mileage > 1000]
    full_car_info_new = full_car_info_new[pd.notnull(full_car_info_new['enginesize'])]

    return full_car_info_new

#print to indicate we are fetching the data
print("Getting model info")
#execute the fetch 
#dump the output to CSV
#COMMENTED OUT WHEN JUST RUNNING PARSE
#full_car_info = get_data(makes_list)
#full_car_info.to_csv("full_car_info.csv")

#get it back from CSV. This ensures that once I've run the section above
#I don't need to do complete scrape again. The processing can be done from this point on 
#using the csv. I should only need to scrape again if I need new data
full_car_info_rd = pd.read_csv('full_car_info.csv')

#remove the Unnamed axis
full_car_info_rd = full_car_info_rd.drop('Unnamed: 0',axis=1)

#Print some of the more interesting counts
print(full_car_info_rd.county.value_counts())
print(full_car_info_rd.enginesize.value_counts())
print(full_car_info_rd.year.value_counts())
print(full_car_info_rd.colour.value_counts())

diesel_cars = full_car_info_rd[(full_car_info_rd.fuel == 'Diesel')]
petrol_cars = full_car_info_rd[(full_car_info_rd.fuel == 'Petrol')]

###### PIE chart for Colours ########
explode=(0.1, 0, 0, 0, 0, 0, 0, 0)
rcParams['font.size'] = 11.0

colour_counts = pd.DataFrame(full_car_info_rd.colour.value_counts())
figure(1, figsize=(6,6))
colors=('Silver', 'Black', 'Blue', 'Grey', 'Red', 'White', 'Brown', 'Green')
pie_colours = pie(colour_counts[:8], labels=colors, colors=colors, explode=explode,
                autopct='%1.1f%%')
pie_colours[2][1].set_color('w')

title('Most popular colours', bbox={'facecolor':'0.8', 'pad':5})
savefig('colours.png', dpi=600)
show()

###### PIE chart for Engine Size ########
engine_counts = pd.DataFrame(full_car_info_rd.enginesize.value_counts())
figure(1, figsize=(6,6))
colors=('Silver', 'cyan', 'Blue', 'Grey', 'Red', 'White', 'Brown', 'Green') 
pie(engine_counts[:8], labels=engine_counts.index[:8], colors=colors, explode=explode,
                autopct='%1.1f%%')
               
title('Most popular engine sizes', bbox={'facecolor':'0.8', 'pad':5})
savefig('engine_size.png', dpi=600)
show()

###### PIE chart for Diesel Engine Size ########
diesel_engine_counts = pd.DataFrame(diesel_cars.enginesize.value_counts())
figure(1, figsize=(6,6))
pie(diesel_engine_counts[:8], labels=diesel_engine_counts.index[:8], explode=explode,
                autopct='%1.1f%%')
               
title('Most popular diesel engine sizes', bbox={'facecolor':'0.8', 'pad':5})
savefig('engine_size_diesel.png', dpi=600)
show()

###### PIE chart for Petrol Engine Size ########
petrol_engine_counts = pd.DataFrame(petrol_cars.enginesize.value_counts())
figure(1, figsize=(6,6))
pie(petrol_engine_counts[:8], labels=petrol_engine_counts.index[:8], explode=explode,
                autopct='%1.1f%%')
               
title('Most popular petrol engine sizes', bbox={'facecolor':'0.8', 'pad':5})
savefig('engine_size_petrol.png', dpi=600)
show()

#print some interesting values to use in the report
mean_prices = (pd.pivot_table(full_car_info_rd, 'price', index='make'))
print((full_car_info_rd.groupby('make').mean()).astype(int))
print((full_car_info_rd.groupby('fuel').mean()).astype(int))
county_grouping = ((full_car_info_rd.groupby('county').mean()).astype(int))
print((full_car_info_rd.groupby('colour').mean()).astype(int))

#show car colour percentages per county
cork_cars = full_car_info_rd[(full_car_info_rd.county == 'Cork')]
print("Cork car colours percentage")
print(((cork_cars.colour.value_counts(normalize=True))*100).astype(int))
dublin_cars = full_car_info_rd[(full_car_info_rd.county == 'Dublin')]
print("Dublin car colours percentage")
print(((dublin_cars.colour.value_counts(normalize=True))*100).astype(int))
galway_cars = full_car_info_rd[(full_car_info_rd.county == 'Galway')]
print("Galway car colours percentage")
print(((galway_cars.colour.value_counts(normalize=True))*100).astype(int))
tip_cars = full_car_info_rd[(full_car_info_rd.county == 'Tipperary')]
print("Tippearary car colours percentage")
print(((tip_cars.colour.value_counts(normalize=True))*100).astype(int))
meath_cars = full_car_info_rd[(full_car_info_rd.county == 'Meath')]
print(((meath_cars.colour.value_counts(normalize=True))*100).astype(int))

#remove some not int values
full_car_info_rd.enginesize = full_car_info_rd.enginesize.replace('under 1.0', '0.9')
full_car_info_rd[['enginesize']] = full_car_info_rd[['enginesize']].astype(float)

#print some correlation values to use in the report
print("Age vs Price correlation: ",round(full_car_info_rd['year'].corr(full_car_info_rd['price']),3))
print("Mileage vs Price correlation: ", round(full_car_info_rd['mileage'].corr(full_car_info_rd['price']),3))
print("Engine Size vs Price correlation: ",round(full_car_info_rd['enginesize'].corr(full_car_info_rd['price']),3))

#compare diesel engine sizes by year
diesel_cars.enginesize = diesel_cars.enginesize.replace('under 1.0', '0.9')
diesel_cars[['enginesize']] = diesel_cars[['enginesize']].astype(float)
diesel_group = ((diesel_cars.groupby('year').mean()).astype(float))

#plot diesel engine size against age
x = diesel_group.index
y = diesel_group.enginesize
m, b = np.polyfit(x, y, 1)
plt.plot(x, y, '.')
plt.plot(x, m*x + b, '-')
plt.xlabel('Year')
plt.ylabel('Engine Size')
savefig('engine_size_diesel_graph.png', dpi=600)
show()

print (county_grouping.sort(['mileage'], ascending=[0]))




